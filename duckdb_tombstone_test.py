# -*- coding: utf-8 -*-
"""duckdb_tombstone_test

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h13VKDJKbqFolu2kPuZq8uHXwjrnyTOv
"""

#!/usr/bin/env python3

import os, textwrap, random, string
from datetime import datetime, timedelta
from time import perf_counter, sleep
import duckdb
import re
import shutil
import time
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression
import numpy as np

PID = os.getpid()

CREATE_t0_TABLE = textwrap.dedent(
    """CREATE TABLE IF NOT EXISTS t0
    (
        bid INT PRIMARY KEY,
        ask DOUBLE NOT NULL
    );"""
)

INSERT_t0 = textwrap.dedent(
    """INSERT INTO t0 (bid, ask)
    VALUES (?,?);"""
)

def generate_random_strings():
    while True:
        random_string = ''.join(random.choices(string.ascii_uppercase, k=3))
        yield random_string

def generate_next_timestamp():
    ts = datetime.utcnow()
    while True:
        yield ts
        ts += timedelta(milliseconds=100)

def generate_random_float():
    while True:
        yield round(random.uniform(0.8, 1.2), 4)

con = duckdb.connect()

def get_table_row_count(table_name):
    result = con.execute(f"SELECT COUNT(*) FROM {table_name}")
    row_count = result.fetchone()[0]
    return row_count

def find_largest_bid(table_name):
    result = con.execute(f"SELECT MAX(bid) FROM {table_name}")
    largest_bid = result.fetchone()[0]
    if get_table_row_count(table_name) == 0:
        return 0
    return largest_bid

def add_rows_t0(n: int):
    k = find_largest_bid("t0")
    rows = [(i, 11) for i in range(k + 1, k + n + 1)]
    con.executemany(INSERT_t0, rows)

def new_latency():
    export_path = '/content/drive/MyDrive/db_export'
    if not os.path.exists(export_path):
        os.makedirs(export_path)

    for filename in os.listdir(export_path):
        file_path = os.path.join(export_path, filename)
        if os.path.isfile(file_path):
            os.unlink(file_path)

    con.execute(f"EXPORT DATABASE '{export_path}'")
    start_time = time.perf_counter()
    add_rows_t0(1)
    end_time = time.perf_counter()
    con.execute("DROP TABLE t0")
    con.execute(f"IMPORT DATABASE '{export_path}'")
    elapsed_time = end_time - start_time

    return elapsed_time

def delete_last_percent(table_name, percentage, i):
    row_count = get_table_row_count(table_name)
    percentage = random.randint(1, percentage)
    delete_row_count = int(row_count * percentage / 100)

    if delete_row_count == 0:
        print("No rows to delete based on the percentage calculated.")
        return None

    rows_to_delete = random.sample(range(1, row_count + 1), delete_row_count)
    delete_statement = f"DELETE FROM {table_name} WHERE ROWID IN ({', '.join('?' for _ in range(delete_row_count))});"
    con.execute(delete_statement, rows_to_delete)

    if i == 1:
        print(f"Deleted {delete_row_count} rows from the table {table_name} and {percentage}%")

    return [rows_to_delete, delete_row_count, percentage]

def delete_last_percent_given(table_name, i, k):
    if k is None or k[1] == 0:
        return
    delete_statement = f"DELETE FROM {table_name} WHERE ROWID IN ({', '.join('?' for _ in range(k[1]))});"
    con.execute(delete_statement, k[0])
    if i == 1:
        print(f"Deleted {k[1]} rows from the table {table_name} and {k[2]}%")

def basic_search():
    add_rows_t0(10000)
    cur = 0
    real = new_latency()
    for a in range(1, 6):
        export_path = '/content/drive/MyDrive/db_export'

        for filename in os.listdir(export_path):
            file_path = os.path.join(export_path, filename)
            if os.path.isfile(file_path):
                os.unlink(file_path)

        con.execute(f"EXPORT DATABASE '{export_path}'")
        k = delete_last_percent("t0", a, 0)
        delete_write_latency = new_latency()
        con.execute("DROP TABLE t0")
        con.execute(f"IMPORT DATABASE '{export_path}'")
        if delete_write_latency > cur and k is not None:
            cur = delete_write_latency
            use = k
    if cur > real and 'use' in locals():
        delete_last_percent_given("t0", 1, use)
        return new_latency()
    else:
        print("No deletion needed, continue")
    return real

def basic_search_ml():
    add_rows_t0(10000)
    initial_latency = new_latency()
    export_path = '/content/drive/MyDrive/db_export'
    data = []
    for percent in range(1, 6):
        con.execute(f"EXPORT DATABASE '{export_path}'")
        k = delete_last_percent("t0", percent, 0)
        if k is None:
            continue
        delete_write_latency = new_latency()
        data.append((percent, delete_write_latency))
        con.execute("DROP TABLE t0")
        con.execute(f"IMPORT DATABASE '{export_path}'")

    if len(data) == 0:
        print("No valid data collected, skipping ML search")
        return initial_latency

    df = pd.DataFrame(data, columns=['percent', 'latency'])
    X = df[['percent']]
    y = df['latency']

    model = LinearRegression()
    model.fit(X, y)

    percent_range = pd.DataFrame(np.arange(1, 6), columns=['percent'])
    predictions = model.predict(percent_range)
    optimal_percent = percent_range.iloc[np.argmax(predictions)].values[0]

    if np.max(predictions) > initial_latency:
        delete_last_percent_given("t0", 1, [[], 0, optimal_percent])
        return new_latency()
    else:
        print("No deletion needed, continue")
        return initial_latency

iterations = 100
latencies = []

for i in range(1):
    con = duckdb.connect()
    con.execute(CREATE_t0_TABLE)
    cnt = 0
    step = 10000

    for a in range(iterations):
        cnt += step
        elapsed_time = basic_search()
        print(f"Write latency is: {elapsed_time:.6f} seconds. Count: {cnt/step}")
        latencies.append(elapsed_time)
        if elapsed_time > 0.01:
            print("SPIKE")

con.execute("DROP TABLE t0;")

plt.plot(range(iterations), latencies, marker='o')
plt.xlabel('Iteration')
plt.ylabel('Write Latency (seconds)')
plt.title('Iteration vs. Write Latency')
plt.grid(True)
plt.show()